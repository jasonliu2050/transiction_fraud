{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will show how to train a complete models in some popular tools: random forest regressor, xgboost and lightGBM\n",
    "\n",
    "    Instructor: Yimin Nie\n",
    "    Email: ymnie888@gmail.com\n",
    "    \n",
    "    In the notebook, I show you the entire pipeline using taxi trip data set, and show how to put all workable codes into \n",
    "    a python project to run your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import useful libs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gc\n",
    "import math\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:\\\\Users\\\\enxxmnx\\\\OneDrive - Ericsson AB\\\\workspace\\\\training\\\\data\\\\taxi_trip\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols = ['pickup_datetime', 'dropoff_datetime', 'store_and_fwd_flag', 'pickup_longitude',\n",
    "           'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "           'trip_duration'\n",
    "           ]\n",
    "df = pd.read_csv(data_path + '\\\\train.zip', compression='zip', usecols=usecols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. process the data and extract features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(lon1, lat1, lon2, lat2):\n",
    "    radius = 6371  # km\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dlat / 2) * math.sin(dlat / 2) + math.cos(math.radians(lat1)) \\\n",
    "        * math.cos(math.radians(lat2)) * math.sin(dlon / 2) * math.sin(dlon / 2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    d = radius * c\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds()\n",
    "df['date'] = df['pickup_datetime'].dt.date\n",
    "condition = (df['pickup_longitude'] != 0) & (df['pickup_latitude'] != 0) & (df['dropoff_longitude'] != 0) \\\n",
    "            & (df['dropoff_latitude'] != 0) & (df['trip_duration'] > 0)\n",
    "df = df.loc[condition]\n",
    "df.sort_values('pickup_datetime', inplace=True, ascending=True)\n",
    "df['month'] = df['pickup_datetime'].dt.month\n",
    "df['day'] = df['pickup_datetime'].dt.day\n",
    "df['hour'] = df['pickup_datetime'].dt.hour\n",
    "df['min'] = df['pickup_datetime'].dt.minute\n",
    "df['dow'] = df['pickup_datetime'].dt.weekday\n",
    "df['doy'] = df['pickup_datetime'].dt.dayofyear\n",
    "df.drop(['pickup_datetime', 'dropoff_datetime'], axis=1, inplace=True)\n",
    "df['store_and_fwd_flag'] = df['store_and_fwd_flag'].map({'N': 0, 'Y': 1})\n",
    "features = ['store_and_fwd_flag', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', \\\n",
    "            'dropoff_latitude', 'passenger_count', 'month', 'day', 'hour', 'min', 'dow', 'doy'\n",
    "            ]\n",
    "\n",
    "df['dist'] = df[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']].apply(\n",
    "    lambda x: distance(x[0], x[1], x[2], x[3]), axis=1)\n",
    "test = df.loc[df.date >= datetime.date(2016, 6, 1)]\n",
    "train = df.loc[df.date < datetime.date(2016, 6, 1)]\n",
    "y_train = np.log1p(train['trip_duration']).values\n",
    "X_train = train[features].reset_index(drop=True)\n",
    "del train, df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build your models\n",
    "\n",
    "\n",
    "    before building your models, make sure \n",
    "        (1) your target ( regression or classification)\n",
    "        (2) evaluation metric in terms of your target\n",
    "        (3) how to train your model (here I use 5-fold cross validation)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lgb():\n",
    "    params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.02,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 10,\n",
    "    'verbose': -1,\n",
    "    'silent':-1,\n",
    "    \"max_depth\": 10,\n",
    "    \"num_leaves\": 128,\n",
    "    \"max_bin\": 512,\n",
    "    \"n_estimators\": 100000\n",
    "}\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    return model\n",
    "\n",
    "def model_xgb():\n",
    "    model = xgb.XGBRegressor(colsample_bytree=0.4,\n",
    "                     gamma=0,                 \n",
    "                     learning_rate=0.07,\n",
    "                     max_depth=3,\n",
    "                     min_child_weight=1.5,\n",
    "                     n_estimators=10000,                                                                    \n",
    "                     reg_alpha=0.75,\n",
    "                     reg_lambda=0.45,\n",
    "                     subsample=0.6,\n",
    "                     seed=42\n",
    "                ) \n",
    "    return model\n",
    "\n",
    "def model_rf():\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=config.n_estimator,\n",
    "        max_depth = config.max_depth,\n",
    "        random_state=config.seed,\n",
    "        n_jobs=config.n_jobs,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use k-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(5)\n",
    "cv_scores = []\n",
    "model_name = 'xgb'\n",
    "for i, (tr_idx, vl_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "    print('FOLD {} \\n'.format(i))\n",
    "    X_tr, y_tr = X_train.loc[tr_idx], y_train[tr_idx]\n",
    "    X_vl, y_vl = X_train.loc[vl_idx], y_train[vl_idx]\n",
    "\n",
    "    if model_name == 'lgb':\n",
    "        model = model_lgb()\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_vl, y_vl)], \\\n",
    "                  eval_metric='rmse', verbose=200, early_stopping_rounds=500)\n",
    "        with open('lgb_model_{}.pkl'.format(i), 'wb') as handle:\n",
    "            pickle.dump(model, handle)\n",
    "        del model, X_tr, X_vl\n",
    "        gc.collect()\n",
    "        \n",
    "    if model_name == 'rf':\n",
    "        model = model_rf()\n",
    "        model.fit(X_tr, y_tr)\n",
    "        with open('rf_model_{}.pkl'.format(i), 'wb') as handle:\n",
    "            pickle.dump(model, handle)\n",
    "        del model, X_tr, X_vl\n",
    "        gc.collect()\n",
    "        \n",
    "    if model_name == 'xgb':\n",
    "        model = model_xgb()\n",
    "        train_data  = xgb.DMatrix(X_tr, label=y_tr)\n",
    "        valid_data  = xgb.DMatrix(X_vl, label=y_vl)\n",
    "        evallist = [(train_data, 'train'), (valid_data, 'valid')]\n",
    "        parms = {'max_depth':15, #maximum depth of a tree 8 12\n",
    "         'objective':'reg:linear',\n",
    "         'eta'      :0.05, #0.3\n",
    "         'subsample':0.9,#SGD will use this percentage of data 0.8 0.99\n",
    "         'lambda '  :3, #L2 regularization term,>1 more conservative 4 \n",
    "         'colsample_bytree ':0.6, #0.9\n",
    "         'colsample_bylevel':0.7, #1 0.7\n",
    "         'min_child_weight': 0.5, #10 0.5\n",
    "         #'nthread'  :3 ... default is max cores\n",
    "         'eval_metric':'rmse'}  #number of cpu core to use\n",
    "        # running for 2k iterations \n",
    "        model = xgb.train(parms, train_data, num_boost_round=2000, evals = evallist,\n",
    "                          early_stopping_rounds=50, maximize=False, \n",
    "                          verbose_eval=100)\n",
    "#         model.fit(X_tr, y_tr,eval_set=(X_vl, y_vl))\n",
    "        with open('rf_model_{}.pkl'.format(i), 'wb') as handle:\n",
    "            pickle.dump(model, handle)\n",
    "        del model, X_tr, X_vl\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
