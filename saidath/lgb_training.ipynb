{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TygaBRii/opt/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import pickle\n",
    "import gc\n",
    "import datetime\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\"\"\"\n",
    "all parameters and configurations\n",
    "\"\"\"\n",
    "train_path='/Users/TygaBRii/Downloads/ML/train_transaction.csv'\n",
    "identity_path = '/Users/TygaBRii/Downloads/ML/train_identity.csv'\n",
    "\n",
    "n_estimator = 100\n",
    "max_depth = 10\n",
    "seed = 10\n",
    "n_jobs = -1\n",
    "stop_rounds = 1000\n",
    "k_folds = 5\n",
    "verbose = 500\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path) # this is main table\n",
    "    return df\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        #'metric':'auc',\n",
    "        'learning_rate': 0.20,\n",
    "        #'is_unbalance': 'true',\n",
    "        'scale_pos_weight': 9, # because training data is extremely unbalanced\n",
    "        'nthread': 4,\n",
    "        'metric': 'binary_logloss'\n",
    "    }\n",
    "\n",
    "\n",
    "def model_lgb_classifier():\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "    return model\n",
    "\n",
    "def model_lgb_default():\n",
    "    model = lgb.LGBMClassifier()\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_KNeighborsClassifier():\n",
    "    model = KNeighborsClassifier(n_neighbors=10, n_jobs=4)\n",
    "\n",
    "    return model\n",
    "\n",
    "def drop_corr_column(df):\n",
    "    print(\"Start drop corr columns ...\")\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Find index of feature columns with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    # Drop features\n",
    "    df.drop(df[to_drop], axis=1, inplace=True)\n",
    "    print(df.info())\n",
    "\n",
    "# Function to clean the names\n",
    "def assign_region(email_addr):\n",
    "    REGION = {\n",
    "        \".jp\": \"Japan\",\n",
    "        \".fr\": \"French\",\n",
    "        \".uk\": \"UK\",\n",
    "        \".mx\": \"Mexico\",\n",
    "        \".de\": \"German\",\n",
    "        \".es\": \"Spain\",\n",
    "        \".com\": \"Global\",\n",
    "        \".net\": \"Global\"\n",
    "    }\n",
    "    for key in REGION.keys():\n",
    "        if email_addr.find(key) != -1:\n",
    "            return REGION[key]\n",
    "\n",
    "\n",
    "def handl_P_emaildomain(train_df):\n",
    "    print(\"start to handle  P_emaildomain...\")\n",
    "    # process NaN value\n",
    "    train_df['P_emaildomain'].fillna('TBD', inplace=True)\n",
    "    #create a new column\n",
    "    train_df = train_df.assign(Region_emaildomain=train_df['P_emaildomain'])\n",
    "    # process P_emaildomain column\n",
    "    train_df.loc[train_df['P_emaildomain'] == 'TBD', 'Region_emaildomain'] = 'Global'\n",
    "    train_df.loc[train_df['P_emaildomain'] == 'yahoo', 'P_emaildomain'] = 'yahoo.com'\n",
    "    train_df.loc[train_df['P_emaildomain'] == 'gmail', 'P_emaildomain'] = 'gmail.com'\n",
    "    train_df['Region_emaildomain'] = train_df['Region_emaildomain'].apply(assign_region)\n",
    "    print(train_df['Region_emaildomain'].head())\n",
    "\n",
    "def handle_NaN(train_df):\n",
    "    # Check for Nan amount in every column\n",
    "    print(\"start to handle NaN ...\")\n",
    "    nan_info = pd.DataFrame(train_df.isnull().sum()).reset_index()\n",
    "    nan_info.columns = ['col', 'nan_cnt']\n",
    "    nan_info.sort_values(by='nan_cnt', ascending=False, inplace=True)\n",
    "    # Columns with missing values\n",
    "    cols_with_missing = nan_info.loc[nan_info.nan_cnt > 0].col.values\n",
    "    # Fill missing values (numbers) with the median\n",
    "    for f in cols_with_missing:\n",
    "        if str(train_df[f].dtype) != 'object':\n",
    "            train_df[f].fillna(train_df[f].median(), inplace=True)\n",
    "    # Fill missing values (objects) with Unknown\n",
    "    for f in cols_with_missing:\n",
    "        if str(train_df[f].dtype) == 'object':\n",
    "            train_df[f].fillna('Unknown', inplace=True)\n",
    "    # Check if there are still Nan values\n",
    "    nan_info = pd.DataFrame(train_df.isnull().sum()).reset_index()\n",
    "    nan_info.columns = ['col', 'nan_cnt']\n",
    "    nan_info.sort_values(by='nan_cnt', ascending=False, inplace=True)\n",
    "    print(nan_info)\n",
    "\n",
    "def transfer_cat_2_int(train_df):\n",
    "    print(\"Start transfer categorical values to integer ...\")\n",
    "    category_columns = train_df.select_dtypes(include=['category', object]).columns\n",
    "    for f in category_columns:\n",
    "        train_df[f] = train_df[f].astype(str)\n",
    "        le = LabelEncoder()\n",
    "        train_df[f] = le.fit_transform(train_df[f])\n",
    "\n",
    "def train_KNeighborsClassifier(X_train, y_train):\n",
    "    print(\"Start to train KNeighborsClassifier ....\")\n",
    "    train = X_train.to_numpy()\n",
    "    target = y_train.to_numpy()\n",
    "\n",
    "    y = target\n",
    "    X = train\n",
    "    # Scaling data (KNeighbors methods do not scale automatically!)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    scaled_features = scaler.transform(X)\n",
    "    # Splitting dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_features, y, test_size=0.35)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=10, n_jobs=4)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_predicted = knn.predict(X_test)\n",
    "    f1_scores = f1_score(y_test, y_predicted, average=\"macro\")\n",
    "    error_rate = np.mean(y_predicted != y_test)\n",
    "    return f1_scores, error_rate\n",
    "\n",
    "def drop_selected_feature(train_txn):\n",
    "    features = ['V322', 'V323', 'V326', 'V330', 'V331', 'V333', 'V334', 'V336', 'V337', 'V339',\n",
    "     'V2', 'V4', 'V6', 'V8', 'V11', 'V324', 'V329', 'V300', 'V10', 'V16', 'V17', 'V22', 'V27', 'V29', 'V31', 'V33',\n",
    "     'V35', 'V39', 'V42', 'V48', 'V51', 'V57', 'V59', 'V63', 'V69', 'V71', 'V73', 'V80', 'V84', 'V90', 'V92', 'V96', 'V103', 'V105', 'V127',\n",
    "     'V139', 'V148', 'V153', 'V155', 'V150', 'V178', 'V182', 'V192', 'V204', 'V212', 'V219', 'V224', 'V233', 'V248',\n",
    "     'V221', 'V238', 'V250', 'V255', 'V272', 'V295', 'V299', 'V308', 'V318']\n",
    "    for f in features:\n",
    "        train_txn.drop([f], axis=1,inplace=True)\n",
    "\n",
    "\n",
    "def other_feature_engineering(train_txn):\n",
    "    train_txn['TransactionDT']= datetime.datetime.fromtimestamp(train_txn['TransactionDT'][0]).strftime(\"%A, %B %d, %Y %I:%M:%S\")\n",
    "\n",
    "    cut_labels = ['1', '2', '3', '4']\n",
    "    cut_bins = [0, 200, 300, 400, train_txn['addr1'].max()]\n",
    "    train_txn['addr1'] = pd.cut(train_txn['addr1'], bins=cut_bins, labels=cut_labels)\n",
    "\n",
    "    cut_labels = ['1', '2', '3', '4', '5']\n",
    "    cut_bins = [0, 20, 40, 60, 80, train_txn['addr2'].max()]\n",
    "    train_txn['addr2'] = pd.cut(train_txn['addr2'], bins=cut_bins, labels=cut_labels)\n",
    "\n",
    "    cut_labels = ['1', '2', '3', '4', '5', '6']\n",
    "    cut_bins = [0, 4001, 7001, 10000, 13000, 16000, train_txn['card1'].max()]\n",
    "    train_txn['card1'] = pd.cut(train_txn['card1'], bins=cut_bins, labels=cut_labels)\n",
    "\n",
    "    cut_labels = ['1', '2', '3', '4', '5']\n",
    "    cut_bins = [0, 200, 300, 400, 500, train_txn['card2'].max()]\n",
    "    train_txn['card2'] = pd.cut(train_txn['card2'], bins=cut_bins, labels=cut_labels)\n",
    "\n",
    "\n",
    "###############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-22 11:19:16.265501\n",
      "Start to load datasets ...\n",
      "(10000, 394) (10000, 41)\n",
      "Drop target column, merge two tables. the new shape of main table: (10000, 394)\n",
      "start to handle  P_emaildomain...\n",
      "0      None\n",
      "1    Global\n",
      "2    Global\n",
      "3    Global\n",
      "4    Global\n",
      "Name: Region_emaildomain, dtype: object\n",
      "start to handle NaN ...\n",
      "               col  nan_cnt\n",
      "0    TransactionID        0\n",
      "285           V232        0\n",
      "296           V243        0\n",
      "295           V242        0\n",
      "294           V241        0\n",
      "..             ...      ...\n",
      "141            V88        0\n",
      "140            V87        0\n",
      "139            V86        0\n",
      "138            V85        0\n",
      "432     DeviceInfo        0\n",
      "\n",
      "[433 rows x 2 columns]\n",
      "Start transfer categorical values to integer ...\n",
      "Memory usage of dataframe is 33.42 MB\n",
      "Memory usage after optimization is: 8.37 MB\n",
      "Decreased by 75.0%\n",
      "Train lightGBM Default\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.2, max_depth=-1,\n",
      "               metric='binary_logloss', min_child_samples=20,\n",
      "               min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "               n_jobs=-1, nthread=4, num_leaves=31, objective='binary',\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               scale_pos_weight=9, silent=True, subsample=1.0,\n",
      "               subsample_for_bin=200000, subsample_freq=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3205\n",
      "           1       0.96      0.57      0.72        95\n",
      "\n",
      "    accuracy                           0.99      3300\n",
      "   macro avg       0.98      0.78      0.85      3300\n",
      "weighted avg       0.99      0.99      0.99      3300\n",
      "\n",
      "[[3203    2]\n",
      " [  41   54]]\n",
      "Train lightGBM with Parameter\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3205\n",
      "           1       0.96      0.51      0.66        95\n",
      "\n",
      "    accuracy                           0.99      3300\n",
      "   macro avg       0.97      0.75      0.83      3300\n",
      "weighted avg       0.98      0.99      0.98      3300\n",
      "\n",
      "[[3203    2]\n",
      " [  47   48]]\n",
      "2020-02-22 11:20:17.431508\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    print(datetime.datetime.now())\n",
    "    print(\"Start to load datasets ...\")\n",
    "    train_df = load_data(train_path)\n",
    "    identity_df = load_data(identity_path)\n",
    "\n",
    "    # # TEST\n",
    "    train_df = train_df[:10000]\n",
    "    identity_df = identity_df[:10000]\n",
    "    # # TEST\n",
    "    print(train_df.shape,identity_df.shape)\n",
    "\n",
    "    print(\"Drop target column, merge two tables. the new shape of main table:\", train_df.shape)\n",
    "    target = train_df['isFraud']\n",
    "    train_df.drop(['isFraud'], axis=1, inplace=True)\n",
    "    train_df = train_df.merge(identity_df, on='TransactionID', how='left')\n",
    "\n",
    "    handl_P_emaildomain(train_df)\n",
    "\n",
    "    handle_NaN(train_df)\n",
    "\n",
    "    other_feature_engineering(train_df)\n",
    "\n",
    "    transfer_cat_2_int(train_df)\n",
    "\n",
    "#    drop_corr_column(train_df)\n",
    "#    drop_selected_feature(train_df)\n",
    "\n",
    "\n",
    "\n",
    "    reduce_mem_usage(train_df)\n",
    "\n",
    "    # train knn model and get result\n",
    "    # f1_scores, error_rate = train_KNeighborsClassifier(train_df, target)\n",
    "    # print(f1_scores, error_rate)\n",
    "\n",
    "    # train knn model and get result\n",
    "    X = train_df\n",
    "    y = target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1000)\n",
    "\n",
    "    # model_name = 'lgb'\n",
    "    # train_model(model_name=model_name, X_train=X_train, y_train=y_train)\n",
    "    # print('model lightGBM training done')\n",
    "    ##################################################################\n",
    "    # fit a lightGBM model to the data\n",
    "    classifiers = [\n",
    "        (\"lightGBM Default\", model_lgb_classifier()),\n",
    "        (\"lightGBM with Parameter\", model_lgb_default())\n",
    "    ]\n",
    "    Accuracy   =[0.77]   # KNN accuracy, need 10 hours to train\n",
    "    for name, model in classifiers:\n",
    "        print(\"Train %s\" % name)\n",
    "        model.fit(X_train, y_train)\n",
    "        print(model)\n",
    "        # make predictions\n",
    "        expected_y = y_test\n",
    "        predicted_y = model.predict(X_test)\n",
    "        # summarize the fit of the model\n",
    "        print(metrics.classification_report(expected_y, predicted_y))\n",
    "        print(metrics.confusion_matrix(expected_y, predicted_y))\n",
    "        accuracy = metrics.classification_report(expected_y, predicted_y, output_dict=True)\n",
    "        Accuracy.append(accuracy[\"accuracy\"])\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "    algs = ['','KNN', 'LGB_Default', 'LGB_With_Paras']\n",
    "    num = [1,2,3]\n",
    "    x_pos =  [i for i, _ in enumerate(algs)]\n",
    "    plt.bar( num, Accuracy, color=[\"g\",\"b\",\"y\"],width = 0.5)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Comparison of Machine learning Algorithm\")\n",
    "    plt.xticks(x_pos, algs)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/TygaBRii/opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - py-xgboost\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-4.8.2                |           py37_0         3.0 MB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.0 MB\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  conda                                         conda-forge --> anaconda\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "conda-4.8.2          | 3.0 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c anaconda py-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: \\ \n",
      "Warning: 8 possible package resolutions (only showing differing packages):\n",
      "  - anaconda/osx-64::ca-certificates-2019.8.28-0, anaconda/osx-64::certifi-2019.9.11-py37_0, anaconda/osx-64::openssl-1.1.1d-h1de35cc_2\n",
      "  - anaconda/osx-64::certifi-2019.9.11-py37_0, anaconda/osx-64::openssl-1.1.1d-h1de35cc_2, defaults/osx-64::ca-certificates-2019.8.28-0\n",
      "  - anaconda/osx-64::ca-certificates-2019.8.28-0, anaconda/osx-64::certifi-2019.9.11-py37_0, defaults/osx-64::openssl-1.1.1d-h1de35cc_2\n",
      "  - anaconda/osx-64::certifi-2019.9.11-py37_0, defaults/osx-64::ca-certificates-2019.8.28-0, defaults/osx-64::openssl-1.1.1d-h1de35cc_2\n",
      "  - anaconda/osx-64::ca-certificates-2019.8.28-0, defaults/osx-64::certifi-2019.9.11-py37_0, defaults/osx-64::openssl-1.1.1d-h1de35cc_2\n",
      "  - defaults/osx-64::ca-certificates-2019.8.28-0, defaults/osx-64::certifi-2019.9.11-py37_0, defaults/osx-64::openssl-1.1.1d-h1de35cc_2\n",
      "  - anaconda/osx-64::ca-certificates-2019.8.28-0, anaconda/osx-64::openssl-1.1.1d-h1de35cc_2, defaults/osx-64::certifi-2019.9.11-py37_0\n",
      "  - anaconda/osx-64::openssl-1.1.1d-h1de35cc_2, defaults/osx-64::ca-certificates-2019.8.28-0, defaults/osx-64::certifi-2019.9.11-py37done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/TygaBRii/opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - lightgbm\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-4.8.2                |           py37_0         3.0 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.0 MB\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  conda                                            anaconda --> conda-forge\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "conda-4.8.2          | 3.0 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgb model\n",
    "def model_lgb():\n",
    "    params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'learning_rate': 0.20,\n",
    "   #'is_unbalance': 'true',\n",
    "    'scale_pos_weight': 9, # because training data is extremely unbalanced\n",
    "    'nthread': 4,\n",
    "    'metric': 'binary_logloss',\n",
    "    \"max_bin\": 512, # Not too small or too big\n",
    "    'feature_fraction': 0.85, # Prevents overfitting.\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 10, # Every 10 runs the lgb will do bagging with the bagging fract portion\n",
    "    'verbose': -1, # -1 because we want to print\n",
    "    'silent':-1,\n",
    "    \"max_depth\": 10,\n",
    "    \"num_leaves\": 128, # Not too small\n",
    "    \"max_bin\": 512, # Not too small or too big\n",
    "    \"n_estimators\": 10000 # How many runs\n",
    "}\n",
    "    \"\"\"\n",
    "    Check parameters explanation on : https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-\n",
    "    what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n",
    "    \"\"\"\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    return model\n",
    "\n",
    "#XGBoost model building\n",
    "def model_xgb():\n",
    "    model = xgb.XGBRegressor(colsample_bytree=0.4,\n",
    "                     gamma=0,                 \n",
    "                     learning_rate=0.07,\n",
    "                     max_depth=3,\n",
    "                     min_child_weight=1.5,\n",
    "                     n_estimators=10000,                                                                    \n",
    "                     reg_alpha=0.75,\n",
    "                     reg_lambda=0.45,\n",
    "                     subsample=0.6,\n",
    "                     seed=42\n",
    "                ) \n",
    "    return model\n",
    "\n",
    "#Random forest model\n",
    "def model_rf():\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators= 100,  #config.n_estimator,\n",
    "        max_depth = 10, #config.max_depth,\n",
    "        random_state=400, #config.seed,\n",
    "        n_jobs=config.n_jobs,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0 \n",
      "\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[200]\ttraining's rmse: 0.32753\ttraining's binary_logloss: 3.68475\tvalid_1's rmse: 0.122169\tvalid_1's binary_logloss: 0.515504\n",
      "[400]\ttraining's rmse: 0.32753\ttraining's binary_logloss: 3.68475\tvalid_1's rmse: 0.122169\tvalid_1's binary_logloss: 0.515504\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's rmse: 0.197552\ttraining's binary_logloss: 0.235611\tvalid_1's rmse: 0.22458\tvalid_1's binary_logloss: 0.220935\n",
      "FOLD 1 \n",
      "\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[200]\ttraining's rmse: 0.326672\ttraining's binary_logloss: 3.67831\tvalid_1's rmse: 0.326675\tvalid_1's binary_logloss: 3.68585\n",
      "[400]\ttraining's rmse: 0.326672\ttraining's binary_logloss: 3.67831\tvalid_1's rmse: 0.326675\tvalid_1's binary_logloss: 3.68585\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's rmse: 0.207734\ttraining's binary_logloss: 0.205656\tvalid_1's rmse: 0.207914\tvalid_1's binary_logloss: 0.162014\n",
      "FOLD 2 \n",
      "\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[200]\ttraining's rmse: 0.270984\ttraining's binary_logloss: 2.52695\tvalid_1's rmse: 0.313885\tvalid_1's binary_logloss: 3.37147\n",
      "[400]\ttraining's rmse: 0.270984\ttraining's binary_logloss: 2.52695\tvalid_1's rmse: 0.313885\tvalid_1's binary_logloss: 3.37147\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's rmse: 0.206233\ttraining's binary_logloss: 0.209179\tvalid_1's rmse: 0.221279\tvalid_1's binary_logloss: 0.240484\n",
      "FOLD 3 \n",
      "\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[200]\ttraining's rmse: 0.215828\ttraining's binary_logloss: 1.5996\tvalid_1's rmse: 0.248878\tvalid_1's binary_logloss: 2.13934\n",
      "[400]\ttraining's rmse: 0.215828\ttraining's binary_logloss: 1.5996\tvalid_1's rmse: 0.248878\tvalid_1's binary_logloss: 2.13934\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's rmse: 0.110076\ttraining's binary_logloss: 0.116196\tvalid_1's rmse: 0.155686\tvalid_1's binary_logloss: 0.143804\n",
      "FOLD 4 \n",
      "\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[200]\ttraining's rmse: 0.189264\ttraining's binary_logloss: 1.23494\tvalid_1's rmse: 0.187282\tvalid_1's binary_logloss: 1.21145\n",
      "[400]\ttraining's rmse: 0.189264\ttraining's binary_logloss: 1.23494\tvalid_1's rmse: 0.187282\tvalid_1's binary_logloss: 1.21145\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's rmse: 0.173661\ttraining's binary_logloss: 0.278869\tvalid_1's rmse: 0.214505\tvalid_1's binary_logloss: 0.489106\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(5) #Number of folds, splitting the data into 5\n",
    "cv_scores = []\n",
    "model_name = 'lgb'\n",
    "for i, (tr_idx, vl_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "    print('FOLD {} \\n'.format(i))\n",
    "    X_tr, y_tr = X_train.loc[tr_idx], y_train[tr_idx] #training set\n",
    "    X_vl, y_vl = X_train.loc[vl_idx], y_train[vl_idx] #validation set\n",
    "\n",
    "    if model_name == 'lgb':\n",
    "        model = model_lgb()\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_vl, y_vl)], \\\n",
    "                  eval_metric='rmse', verbose=200, early_stopping_rounds=500)\n",
    "        \n",
    "        # rmse - root mean square ; 200 - print something every 200 iterations;\n",
    "        \n",
    "        with open('lgb_model_{}.pkl'.format(i), 'wb') as handle:\n",
    "        #pkl is a python function to save the model\n",
    "            pickle.dump(model, handle)\n",
    "        del model, X_tr, X_vl\n",
    "        gc.collect()\n",
    "        \n",
    "    if model_name == 'rf':\n",
    "        model = model_rf()\n",
    "        model.fit(X_tr, y_tr)\n",
    "        with open('rf_model_{}.pkl'.format(i), 'wb') as handle: \n",
    "        #pkl is a python function to save the model\n",
    "            pickle.dump(model, handle)\n",
    "        del model, X_tr, X_vl\n",
    "        gc.collect()\n",
    "        \n",
    "    if model_name == 'xgb':\n",
    "        model = model_xgb()\n",
    "        train_data  = xgb.DMatrix(X_tr, label=y_tr)\n",
    "        valid_data  = xgb.DMatrix(X_vl, label=y_vl)\n",
    "        evallist = [(train_data, 'train'), (valid_data, 'valid')]\n",
    "        parms = {'max_depth':15, #maximum depth of a tree 8 12\n",
    "         'objective':'reg:linear',\n",
    "         'eta'      :0.05, #0.3\n",
    "         'subsample':0.9,#SGD will use this percentage of data 0.8 0.99\n",
    "         'lambda '  :3, #L2 regularization term,>1 more conservative 4 \n",
    "         'colsample_bytree ':0.6, #0.9\n",
    "         'colsample_bylevel':0.7, #1 0.7\n",
    "         'min_child_weight': 0.5, #10 0.5\n",
    "         #'nthread'  :3 ... default is max cores\n",
    "         'eval_metric':'rmse'}  #number of cpu core to use\n",
    "        # running for 2k iterations \n",
    "        model = xgb.train(parms, train_data, num_boost_round=2000, evals = evallist,\n",
    "                          early_stopping_rounds=50, maximize=False, \n",
    "                          verbose_eval=100)\n",
    "#         model.fit(X_tr, y_tr,eval_set=(X_vl, y_vl))\n",
    "        with open('rf_model_{}.pkl'.format(i), 'wb') as handle:\n",
    "            pickle.dump(model, handle)\n",
    "        del model, X_tr, X_vl\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
